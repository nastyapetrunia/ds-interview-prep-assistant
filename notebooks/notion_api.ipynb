{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48047262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from notion_client import Client\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2cfe54b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTION_API_KEY = os.getenv(\"NOTION_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8dc80d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "notion = Client(auth=NOTION_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae3c02c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_databases(notion):\n",
    "    databases = []\n",
    "    cursor = None\n",
    "    while True:\n",
    "        response = notion.search(filter={\"property\": \"object\", \"value\": \"database\"}, start_cursor=cursor)\n",
    "        databases.extend(response[\"results\"])\n",
    "        cursor = response.get(\"next_cursor\")\n",
    "        if not cursor:\n",
    "            break\n",
    "    return databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "790774fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pages_from_database(notion, database_id):\n",
    "    pages = []\n",
    "    cursor = None\n",
    "    while True:\n",
    "        response = notion.databases.query(database_id=database_id, start_cursor=cursor)\n",
    "        pages.extend(response[\"results\"])\n",
    "        cursor = response.get(\"next_cursor\")\n",
    "        if not cursor:\n",
    "            break\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d33809f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_child_pages(notion, parent_id):\n",
    "    pages = []\n",
    "    children = notion.blocks.children.list(parent_id)[\"results\"]\n",
    "\n",
    "    for block in children:\n",
    "        if block[\"type\"] == \"child_page\":\n",
    "            pages.append(block)\n",
    "        if block.get(\"has_children\"):\n",
    "            pages.extend(find_child_pages(notion, block[\"id\"]))\n",
    "\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37869eb1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Step 3: Find nested child pages\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m all_pages:\n\u001b[0;32m---> 15\u001b[0m     child_pages \u001b[38;5;241m=\u001b[39m \u001b[43mfind_child_pages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnotion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpage\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     all_pages\u001b[38;5;241m.\u001b[39mextend(child_pages)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Step 4: Print or process\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m, in \u001b[0;36mfind_child_pages\u001b[0;34m(notion, parent_id)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_child_pages\u001b[39m(notion, parent_id):\n\u001b[1;32m      2\u001b[0m     pages \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 3\u001b[0m     children \u001b[38;5;241m=\u001b[39m \u001b[43mnotion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchildren\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent_id\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m children:\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m block[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchild_page\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/site-packages/notion_client/api_endpoints.py:35\u001b[0m, in \u001b[0;36mBlocksChildrenEndpoint.list\u001b[0;34m(self, block_id, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m, block_id: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SyncAsync[Any]:\n\u001b[1;32m     31\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a paginated array of child [block objects](https://developers.notion.com/reference/block) contained in the block.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m    *[üîó Endpoint documentation](https://developers.notion.com/reference/get-block-children)*\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mblocks/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mblock_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/children\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpick\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstart_cursor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpage_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/site-packages/notion_client/client.py:191\u001b[0m, in \u001b[0;36mClient.request\u001b[0;34m(self, path, method, query, body, auth)\u001b[0m\n\u001b[1;32m    189\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request(method, path, query, body, auth)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 191\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException:\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestTimeoutError()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/site-packages/httpx/_client.py:1014\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1010\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1011\u001b[0m     )\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1014\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1018\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/site-packages/httpx/_transports/default.py:250\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    237\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    238\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    239\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    247\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    248\u001b[0m )\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 250\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    255\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    256\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    257\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    258\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    259\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    253\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    232\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/site-packages/httpcore/_sync/connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/site-packages/httpcore/_sync/http11.py:136\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/site-packages/httpcore/_sync/http11.py:106\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m     99\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    100\u001b[0m     (\n\u001b[1;32m    101\u001b[0m         http_version,\n\u001b[1;32m    102\u001b[0m         status,\n\u001b[1;32m    103\u001b[0m         reason_phrase,\n\u001b[1;32m    104\u001b[0m         headers,\n\u001b[1;32m    105\u001b[0m         trailing_data,\n\u001b[0;32m--> 106\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    114\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/site-packages/httpcore/_sync/http11.py:177\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    174\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/site-packages/httpcore/_sync/http11.py:217\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/site-packages/httpcore/_backends/sync.py:128\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/ssl.py:1232\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1228\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1229\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1230\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1231\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/ssl.py:1105\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1104\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1105\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[1;32m   1107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "databases = get_all_databases(notion)\n",
    "\n",
    "all_pages = []\n",
    "\n",
    "# Step 2: Get pages from databases\n",
    "for db in databases:\n",
    "    db_id = db[\"id\"]\n",
    "    pages = get_pages_from_database(notion, db_id)\n",
    "    all_pages.extend(pages)\n",
    "\n",
    "# Step 3: Find nested child pages\n",
    "for page in all_pages:\n",
    "    child_pages = find_child_pages(notion, page[\"id\"])\n",
    "    all_pages.extend(child_pages)\n",
    "\n",
    "# Step 4: Print or process\n",
    "print(f\"Found {len(all_pages)} pages.\")\n",
    "for page in all_pages:\n",
    "    print(page[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d65f5d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_blocks(notion, block_id):\n",
    "    \"\"\"Recursively fetch all blocks inside a page or block\"\"\"\n",
    "    all_blocks = []\n",
    "\n",
    "    cursor = None\n",
    "    while True:\n",
    "        response = notion.blocks.children.list(block_id=block_id, start_cursor=cursor)\n",
    "        results = response[\"results\"]\n",
    "        all_blocks.extend(results)\n",
    "\n",
    "        for block in results:\n",
    "            if block.get(\"has_children\"):\n",
    "                child_blocks = get_all_blocks(notion, block[\"id\"])\n",
    "                all_blocks.extend(child_blocks)\n",
    "\n",
    "        cursor = response.get(\"next_cursor\")\n",
    "        if not cursor:\n",
    "            break\n",
    "\n",
    "    return all_blocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4204d53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_block(block):\n",
    "    block_type = block[\"type\"]\n",
    "    data = block.get(block_type, {})\n",
    "\n",
    "    text = \"\"\n",
    "\n",
    "    if \"rich_text\" in data:\n",
    "        text = \"\".join([t[\"plain_text\"] for t in data[\"rich_text\"]])\n",
    "\n",
    "    elif block_type == \"child_page\":\n",
    "        text = f\"[Child Page] {data['title']}\"\n",
    "\n",
    "    elif block_type == \"image\":\n",
    "        text = f\"[Image] {data.get('type')}: {data.get('external', {}).get('url', '')}\"\n",
    "\n",
    "    elif block_type == \"code\":\n",
    "        text = f\"[Code ({data.get('language', 'unknown')})]\\n\" + \"\".join([t[\"plain_text\"] for t in data[\"rich_text\"]])\n",
    "\n",
    "    # Add more block types as needed (equation, video, file, etc.)\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7639a66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_page_text(notion, page_id):\n",
    "    blocks = get_all_blocks(notion, page_id)\n",
    "    lines = []\n",
    "\n",
    "    for block in blocks:\n",
    "        text = extract_text_from_block(block)\n",
    "        if text:\n",
    "            lines.append(text)\n",
    "\n",
    "    return \"\\n\".join(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e44cc686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Child Page] Agenda\n",
      "[Child Page] Notes\n",
      "[Child Page] All tasks\n",
      "[Child Page] LangChain for LLM Application Development Notes\n",
      "[Child Page] LangChain: Chat with Your Data Notes\n",
      "Summary: Basics of LangChain\n",
      "[Child Page] Lesson 1: Models, Prompts and parsers\n",
      "[Child Page] Lesson 2: Memory\n",
      "[Child Page] Lesson 3: Chains\n",
      "[Child Page] Lesson 4: Q&A over Documents\n",
      "[Child Page] Lesson 5: Evaluation\n",
      "[Child Page] Lesson 6: Agents\n",
      "üöó¬†Prompt templates\n",
      "ü§î¬†Why: prevent prompt injection, reuse prompts.\n",
      "üìù¬†Important libraries: \n",
      "from langchain.chat_models import ChatOpenAI  # (clickable link to all providers available)\n",
      "from langchain.prompts import ChatPromptTemplate\n",
      "üë©‚Äçüíª¬†Code example:\n",
      "from langchain.chat_models import ChatOpenAI\n",
      "from langchain.prompts import ChatPromptTemplate\n",
      "\n",
      "template_string = \"\"\"Translate the text \\\n",
      "that is delimited by triple backticks \\\n",
      "into a style that is {style}. \\\n",
      "text: ```{text}```\n",
      "\"\"\"\n",
      "\n",
      "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
      "\n",
      "print(prompt_template.messages[0].prompt.input_variables)  # ['style', 'text']\n",
      "\n",
      "customer_style = \"American English in a calm and respectful tone\"\n",
      "customer_email = \"\"\"\n",
      "Arrr, I be fuming that me blender lid \\\n",
      "flew off and splattered me kitchen walls \\\n",
      "with smoothie! And to make matters worse, \\\n",
      "the warranty don't cover the cost of \\\n",
      "cleaning up me kitchen. I need yer help \\\n",
      "right now, matey!\n",
      "\"\"\"\n",
      "\n",
      "customer_messages = prompt_template.format_messages(\n",
      "                    style=customer_style,\n",
      "                    text=customer_email)\n",
      "                    \n",
      " # Call the LLM to translate to the style of the customer message\n",
      "customer_response = chat(customer_messages)\n",
      "üöô¬†Response parsing \n",
      "ü§î¬†Why: get responses from LLM in a structured format.\n",
      "üìù¬†Important libraries: \n",
      "from langchain.output_parsers import ResponseSchema\n",
      "from langchain.output_parsers import StructuredOutputParser\n",
      "üë©‚Äçüíª¬†Code example:\n",
      "from langchain.output_parsers import ResponseSchema\n",
      "from langchain.output_parsers import StructuredOutputParser\n",
      "\n",
      "gift_schema = ResponseSchema(name=\"gift\",\n",
      "                             description=\"\"\"Was the item purchased\\\n",
      "                             as a gift for someone else? \\\n",
      "                             Answer True if yes,\\\n",
      "                             False if not or unknown.\"\"\")\n",
      "delivery_days_schema = ResponseSchema(name=\"delivery_days\",\n",
      "                                      description=\"\"\"How many days\\\n",
      "                                      did it take for the product\\\n",
      "                                      to arrive? If this \\\n",
      "                                      information is not found,\\\n",
      "                                      output -1.\"\"\")\n",
      "price_value_schema = ResponseSchema(name=\"price_value\",\n",
      "                                    description=\"\"\"Extract any\\\n",
      "                                    sentences about the value or \\\n",
      "                                    price, and output them as a \\\n",
      "                                    comma separated Python list.\"\"\")\n",
      "\n",
      "response_schemas = [gift_schema, \n",
      "                    delivery_days_schema,\n",
      "                    price_value_schema]\n",
      "                    \n",
      "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
      "\n",
      "format_instructions = output_parser.get_format_instructions()\n",
      "\n",
      "review_template_2 = \"\"\"\\\n",
      "For the following text, extract the following information:\n",
      "\n",
      "gift: Was the item purchased as a gift for someone else? \\\n",
      "Answer True if yes, False if not or unknown.\n",
      "\n",
      "delivery_days: How many days did it take for the product\\\n",
      "to arrive? If this information is not found, output -1.\n",
      "\n",
      "price_value: Extract any sentences about the value or price,\\\n",
      "and output them as a comma separated Python list.\n",
      "\n",
      "text: {text}\n",
      "\n",
      "{format_instructions}\n",
      "\"\"\"\n",
      "\n",
      "prompt = ChatPromptTemplate.from_template(template=review_template_2)\n",
      "\n",
      "customer_review = \"\"\"\\\n",
      "This leaf blower is pretty amazing.  It has four settings:\\\n",
      "candle blower, gentle breeze, windy city, and tornado. \\\n",
      "It arrived in two days, just in time for my wife's \\\n",
      "anniversary present. \\\n",
      "I think my wife liked it so much she was speechless. \\\n",
      "So far I've been the only one using it, and I've been \\\n",
      "using it every other morning to clear the leaves on our lawn. \\\n",
      "It's slightly more expensive than the other leaf blowers \\\n",
      "out there, but I think it's worth it for the extra features.\n",
      "\"\"\"\n",
      "messages = prompt.format_messages(text=customer_review, \n",
      "                                  format_instructions=format_instructions)\n",
      "                                \n",
      "response = chat(messages)\n",
      "\n",
      "print(response.content)\n",
      "\n",
      "\"\"\"\n",
      "```json\n",
      "{\n",
      "\t\"gift\": true,\n",
      "\t\"delivery_days\": 2,\n",
      "\t\"price_value\": [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]\n",
      "}\n",
      "```\n",
      "\"\"\"\n",
      "\n",
      "output_dict = output_parser.parse(response.content)\n",
      "output_dict.get('delivery_days')  # 2\n",
      "ü§î¬†Why: create chatbots that can respond based on previous interaction with user.\n",
      "üìù¬†Important libraries: \n",
      "from langchain.chains import ConversationChain\n",
      "from langchain.memory import ConversationBufferMemory\n",
      "from langchain.memory import ConversationTokenBufferMemory\n",
      "from langchain.memory import ConversationBufferWindowMemory\n",
      "from langchain.memory import ConversationSummaryBufferMemory\n",
      "üë©‚Äçüíª¬†Code example:\n",
      "from langchain.chat_models import ChatOpenAI\n",
      "from langchain.chains import ConversationChain\n",
      "\n",
      "from langchain.memory import ConversationBufferMemory\n",
      "from langchain.memory import ConversationTokenBufferMemory\n",
      "from langchain.memory import ConversationBufferWindowMemory\n",
      "from langchain.memory import ConversationSummaryBufferMemory\n",
      "\n",
      "'''\n",
      "ConversationBufferMemory \n",
      "Description: remember ALL previous interactions\n",
      "'''\n",
      "memory = ConversationBufferMemory()\n",
      "\n",
      "\n",
      "'''\n",
      "ConversationBufferWindowMemory \n",
      "Description: Remember recent k interactions.\n",
      "Params: \n",
      "\tk: number of most recent interactions to keep in memory\n",
      "'''\n",
      "#  - \n",
      "memory = ConversationBufferWindowMemory(k=2)  # remember 2 most recent interactions\n",
      "\n",
      "\n",
      "'''\n",
      "ConversationTokenBufferMemory \n",
      "Description: Remember all recent interactions that fit within the maximum token limit.\n",
      "Params: \n",
      "\tllm: specify llm to correctly count tokens for the model you are going to use.\n",
      "\tmax_token_limit: maximum number of tokens allowed for the memory.\n",
      "'''\n",
      "#  - \n",
      "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)\n",
      "\n",
      "'''\n",
      "ConversationSummaryBufferMemory \n",
      "Description: Remember all recent interactions that fit within the maximum token limit.\n",
      "Summarize the part that does not fit within the token limit.\n",
      "Params: \n",
      "\tllm: specify llm to correctly count tokens for the model you are going to use.\n",
      "\tmax_token_limit: maximum number of tokens allowed for the memory.\n",
      "'''\n",
      "#  - \n",
      "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)\n",
      "\n",
      "\n",
      "# create conversation with memory\n",
      "llm = ChatOpenAI(temperature=0.0, model=llm_model)\n",
      "memory = ConversationBufferMemory()\n",
      "conversation = ConversationChain(\n",
      "    llm=llm, \n",
      "    memory = memory,\n",
      "    verbose=True  # True for printing intermediate steps, False otherwise\n",
      ")\n",
      "\n",
      "# talk to llm and add convo to memory\n",
      "conversation.predict(input=\"Hi, my name is Andrew\")\n",
      "\n",
      "# see all the history \n",
      "memory.load_memory_variables({})\n",
      "\n",
      "# add convo to memory by hand \n",
      "memory.save_context({\"input\": \"Hi\"}, \n",
      "                    {\"output\": \"What's up\"})\n",
      "ü§î¬†Why: help structure, organize, and scale complex LLM workflows. They give you modularity, reusability, and better control ‚Äî essential for building production-ready AI apps.\n",
      "üìù¬†Important libraries: \n",
      "from langchain.chains import LLMChain\n",
      "from langchain.chains import SequentialChain\n",
      "from langchain.chains import SimpleSequentialChain\n",
      "from langchain.chains.router import MultiPromptChain\n",
      "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
      "\n",
      "from langchain.prompts import PromptTemplate  # working with raw completions (text-davinci-003, llama)\n",
      "from langchain.prompts import ChatPromptTemplate  # when working with chat models that take a sequence of messages (gpt-3.5-turbo, gpt-4, Claude, etc.\n",
      "üë©‚Äçüíª¬†Code example:\n",
      "Prerequisites\n",
      "from langchain.chat_models import ChatOpenAI\n",
      "from langchain.prompts import PromptTemplate\n",
      "from langchain.prompts import ChatPromptTemplate\n",
      "\n",
      "from langchain.chains import LLMChain\n",
      "from langchain.chains import SequentialChain\n",
      "from langchain.chains import SimpleSequentialChain\n",
      "from langchain.chains.router import MultiPromptChain\n",
      "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
      "\n",
      "llm = ChatOpenAI(temperature=0.9, model=llm_model)\n",
      "first_prompt = ChatPromptTemplate.from_template(\n",
      "    \"What is the best name to describe a company that makes {product}?\"\n",
      ")\n",
      "LLMChain\n",
      "'''\n",
      "LLMChain \n",
      "\n",
      "Description: single prompt ‚Üí LLM ‚Üí output chain.\n",
      "It wraps a PromptTemplate and a LLM, making it easy to run inputs through an LLM with structured formatting.\n",
      "\n",
      "Params: \n",
      "\tllm: specify llm to use.\n",
      "\tprompt: ChatPromptTemplate.\n",
      "'''\n",
      "chain_one = LLMChain(llm=llm, prompt=first_prompt)\n",
      "\n",
      "# use the chain\n",
      "product = \"Queen Size Sheet Set\"\n",
      "chain_one.run(product)\n",
      "SimpleSequentialChain\n",
      "'''\n",
      "SimpleSequentialChain \n",
      "\n",
      "Description: connects chains in a simple linear sequence.\n",
      "Use when you have multiple LLMChains, and the output of one becomes the input of the next ‚Äî with no intermediate variables.\n",
      "\n",
      "Params: \n",
      "\tchains: list of LLMChain obj.\n",
      "\tverbose: True to show intermediate steps, False otherwise.\n",
      "'''\n",
      "second_prompt = ChatPromptTemplate.from_template(\n",
      "    \"Write a 20 words description for the following company:{company_name}\"\n",
      ")\n",
      "\n",
      "chain_two = LLMChain(llm=llm, prompt=second_prompt)\n",
      "\n",
      "overall_simple_chain = SimpleSequentialChain(chains=[chain_one, chain_two],\n",
      "                                             verbose=True)\n",
      "                                             \n",
      "# use the chain\n",
      "product = \"Queen Size Sheet Set\"\n",
      "overall_simple_chain.run(product)\n",
      "SequentialChain\n",
      "'''\n",
      "SequentialChain \n",
      "\n",
      "Description: You need more control and flexibility ‚Äî especially if you're working with multiple inputs and outputs, and want to pass data between steps explicitly.\n",
      "\n",
      "Params: \n",
      "'''\n",
      "\n",
      "# prompt template 1: translate to english\n",
      "first_prompt = ChatPromptTemplate.from_template(\n",
      "    \"Translate the following review to english:\"\n",
      "    \"\\n\\n{Review}\"\n",
      ")\n",
      "# chain 1: input= Review and output= English_Review\n",
      "chain_one = LLMChain(llm=llm, prompt=first_prompt, \n",
      "                     output_key=\"English_Review\"\n",
      "                    )\n",
      "                    \n",
      "second_prompt = ChatPromptTemplate.from_template(\n",
      "    \"Can you summarize the following review in 1 sentence:\"\n",
      "    \"\\n\\n{English_Review}\"\n",
      ")\n",
      "# chain 2: input= English_Review and output= summary\n",
      "chain_two = LLMChain(llm=llm, prompt=second_prompt, \n",
      "                     output_key=\"summary\"\n",
      "                    )\n",
      "                    \n",
      "# prompt template 3: translate to english\n",
      "third_prompt = ChatPromptTemplate.from_template(\n",
      "    \"What language is the following review:\\n\\n{Review}\"\n",
      ")\n",
      "# chain 3: input= Review and output= language\n",
      "chain_three = LLMChain(llm=llm, prompt=third_prompt,\n",
      "                       output_key=\"language\"\n",
      "                      )\n",
      "                      \n",
      "# prompt template 4: follow up message\n",
      "fourth_prompt = ChatPromptTemplate.from_template(\n",
      "    \"Write a follow up response to the following \"\n",
      "    \"summary in the specified language:\"\n",
      "    \"\\n\\nSummary: {summary}\\n\\nLanguage: {language}\"\n",
      ")\n",
      "# chain 4: input= summary, language and output= followup_message\n",
      "chain_four = LLMChain(llm=llm, prompt=fourth_prompt,\n",
      "                      output_key=\"followup_message\"\n",
      "                     )\n",
      "                     \n",
      "overall_chain = SequentialChain(\n",
      "    chains=[chain_one, chain_two, chain_three, chain_four],\n",
      "    input_variables=[\"Review\"],\n",
      "    output_variables=[\"English_Review\", \"summary\", \"followup_message\"],\n",
      "    verbose=True\n",
      ")\n",
      "\n",
      "review = \"\"\"\n",
      "\"Je trouve le go√ªt m√©diocre. La mousse ne tient pas, c'est bizarre. \n",
      "J'ach√®te les m√™mes dans le commerce et le go√ªt est bien meilleur...\\n\n",
      "Vieux lot ou contrefa√ßon !?\"\n",
      "\"\"\"\n",
      "overall_chain(review)\n",
      "Router Chain\n",
      "'''\n",
      "MultiPromptChain \n",
      "\n",
      "Description: You want to route input dynamically to different sub-chains based on content (like a switch-case logic).\n",
      "It uses an LLM to decide which chain to run, then routes the input accordingly.\n",
      "\n",
      "Params: \n",
      "\trouter_chain: LLMRouterChain obj.\n",
      "\tdestination_chains: dict where key: str = name of the route(chain), value: LLMChain.\n",
      "\tdefault_chain: LLMChain to route to when llm can't find a good route among existing.\n",
      "\tverbose: True to show intermediate steps, False otherwise.\n",
      "'''\n",
      "# prompt templates for different routes\n",
      "\n",
      "physics_template = \"\"\"You are a very smart physics professor. \\\n",
      "You are great at answering questions about physics in a concise\\\n",
      "and easy to understand manner. \\\n",
      "When you don't know the answer to a question you admit\\\n",
      "that you don't know.\n",
      "\n",
      "Here is a question:\n",
      "{input}\"\"\"\n",
      "\n",
      "\n",
      "math_template = \"\"\"You are a very good mathematician. \\\n",
      "You are great at answering math questions. \\\n",
      "You are so good because you are able to break down \\\n",
      "hard problems into their component parts, \n",
      "answer the component parts, and then put them together\\\n",
      "to answer the broader question.\n",
      "\n",
      "Here is a question:\n",
      "{input}\"\"\"\n",
      "\n",
      "history_template = \"\"\"You are a very good historian. \\\n",
      "You have an excellent knowledge of and understanding of people,\\\n",
      "events and contexts from a range of historical periods. \\\n",
      "You have the ability to think, reflect, debate, discuss and \\\n",
      "evaluate the past. You have a respect for historical evidence\\\n",
      "and the ability to make use of it to support your explanations \\\n",
      "and judgements.\n",
      "\n",
      "Here is a question:\n",
      "{input}\"\"\"\n",
      "\n",
      "\n",
      "computerscience_template = \"\"\" You are a successful computer scientist.\\\n",
      "You have a passion for creativity, collaboration,\\\n",
      "forward-thinking, confidence, strong problem-solving capabilities,\\\n",
      "understanding of theories and algorithms, and excellent communication \\\n",
      "skills. You are great at answering coding questions. \\\n",
      "You are so good because you know how to solve a problem by \\\n",
      "describing the solution in imperative steps \\\n",
      "that a machine can easily interpret and you know how to \\\n",
      "choose a solution that has a good balance between \\\n",
      "time complexity and space complexity. \n",
      "\n",
      "Here is a question:\n",
      "{input}\"\"\"\n",
      "\n",
      "prompt_infos = [\n",
      "    {\n",
      "        \"name\": \"physics\", \n",
      "        \"description\": \"Good for answering questions about physics\", \n",
      "        \"prompt_template\": physics_template\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"math\", \n",
      "        \"description\": \"Good for answering math questions\", \n",
      "        \"prompt_template\": math_template\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"History\", \n",
      "        \"description\": \"Good for answering history questions\", \n",
      "        \"prompt_template\": history_template\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"computer science\", \n",
      "        \"description\": \"Good for answering computer science questions\", \n",
      "        \"prompt_template\": computerscience_template\n",
      "    }\n",
      "]\n",
      "from langchain.chains.router import MultiPromptChain\n",
      "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
      "from langchain.prompts import PromptTemplate\n",
      "\n",
      "llm = ChatOpenAI(temperature=0, model=llm_model)\n",
      "\n",
      "destination_chains = {}\n",
      "for p_info in prompt_infos:\n",
      "    name = p_info[\"name\"]\n",
      "    prompt_template = p_info[\"prompt_template\"]\n",
      "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
      "    chain = LLMChain(llm=llm, prompt=prompt)\n",
      "    destination_chains[name] = chain  \n",
      "    \n",
      "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
      "destinations_str = \"\\n\".join(destinations)\n",
      "\n",
      "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
      "default_chain = LLMChain(llm=llm, prompt=default_prompt)\n",
      "\n",
      "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
      "language model select the model prompt best suited for the input. \\\n",
      "You will be given the names of the available prompts and a \\\n",
      "description of what the prompt is best suited for. \\\n",
      "You may also revise the original input if you think that revising\\\n",
      "it will ultimately lead to a better response from the language model.\n",
      "\n",
      "<< FORMATTING >>\n",
      "Return a markdown code snippet with a JSON object formatted to look like:\n",
      "```json\n",
      "{{{{\n",
      "    \"destination\": string \\ \"DEFAULT\" or name of the prompt to use in {destinations}\n",
      "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
      "}}}}\n",
      "```\n",
      "\n",
      "REMEMBER: The value of ‚Äúdestination‚Äù MUST match one of \\\n",
      "the candidate prompts listed below.\\\n",
      "If ‚Äúdestination‚Äù does not fit any of the specified prompts, set it to ‚ÄúDEFAULT.‚Äù\n",
      "REMEMBER: \"next_inputs\" can just be the original input \\\n",
      "if you don't think any modifications are needed.\n",
      "\n",
      "<< CANDIDATE PROMPTS >>\n",
      "{destinations}\n",
      "\n",
      "<< INPUT >>\n",
      "{{input}}\n",
      "\n",
      "<< OUTPUT (remember to include the ```json)>>\"\"\"\n",
      "\n",
      "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
      "    destinations=destinations_str\n",
      ")\n",
      "router_prompt = PromptTemplate(\n",
      "    template=router_template,\n",
      "    input_variables=[\"input\"],\n",
      "    output_parser=RouterOutputParser(),\n",
      ")\n",
      "\n",
      "router_chain = LLMRouterChain.from_llm(llm, router_prompt)\n",
      "\n",
      "chain = MultiPromptChain(router_chain=router_chain, \n",
      "                         destination_chains=destination_chains, \n",
      "                         default_chain=default_chain, \n",
      "                         verbose=True)\n",
      "                        \n",
      "chain.run(\"What is black body radiation?\")\n",
      "ü§î¬†Why: Answer questions based on the contents of uploaded or indexed documents, not just LLM's general knowledge.\n",
      "üìù¬†Important libraries: \n",
      "from langchain.llms import OpenAI\n",
      "from langchain.chains import RetrievalQA\n",
      "from langchain.chat_models import ChatOpenAI\n",
      "from langchain.document_loaders import CSVLoader\n",
      "from langchain.indexes import VectorstoreIndexCreator\n",
      "from langchain.vectorstores import DocArrayInMemorySearch\n",
      "\n",
      "from langchain.document_loaders import CSVLoader\n",
      "from langchain.embeddings import OpenAIEmbeddings\n",
      "üë©‚Äçüíª¬†Code example:\n",
      "#  Prerequisites\n",
      "from langchain.llms import OpenAI\n",
      "from langchain.chains import RetrievalQA\n",
      "from langchain.chat_models import ChatOpenAI\n",
      "from langchain.document_loaders import CSVLoader\n",
      "from langchain.indexes import VectorstoreIndexCreator\n",
      "from langchain.vectorstores import DocArrayInMemorySearch\n",
      "\n",
      "from langchain.document_loaders import CSVLoader\n",
      "from langchain.embeddings import OpenAIEmbeddings\n",
      "\n",
      "llm_model = \"gpt-3.5-turbo\"\n",
      "Less code example:\n",
      "# load file with data\n",
      "csv_file =  # file path herre\n",
      "loader = CSVLoader(file_path=csv_file)\n",
      "\n",
      "# create a vector store\n",
      "index = VectorstoreIndexCreator(\n",
      "    vectorstore_cls=DocArrayInMemorySearch\n",
      ").from_loaders([loader])\n",
      "\n",
      "query =\"Please list all your shirts with sun protection in a table in markdown and summarize each one.\"\n",
      "\n",
      "\n",
      "llm_replacement_model = OpenAI(temperature=0, \n",
      "                               model='gpt-3.5-turbo-instruct')\n",
      "\n",
      "response = index.query(query, \n",
      "                       llm = llm_replacement_model)\n",
      "                       \n",
      "display(Markdown(response))\n",
      "More detailed example:\n",
      "loader = CSVLoader(file_path=csv_file)\n",
      "docs = loader.load()\n",
      "\n",
      "embeddings = OpenAIEmbeddings()\n",
      "db = DocArrayInMemorySearch.from_documents(\n",
      "    docs, \n",
      "    embeddings\n",
      ")\n",
      "\n",
      "query = \"Please suggest a shirt with sunblocking\"\n",
      "docs = db.similarity_search(query)\n",
      "\n",
      "retriever = db.as_retriever()\n",
      "\n",
      "llm = ChatOpenAI(temperature = 0.1, model=llm_model)\n",
      "qdocs = \"\".join([docs[i].page_content for i in range(len(docs))])\n",
      "\n",
      "prompt = (\n",
      "    f\"{qdocs}\\n\\n\"\n",
      "    \"Question: From the above information, extract all shirts that mention sun protection. \"\n",
      "    \"Present them in a markdown table with columns: Name and Description. \"\n",
      "    \"Do not include any additional text or explanation. Only output the table.\"\n",
      ")\n",
      "\n",
      "response = llm.call_as_llm(prompt) \n",
      "\n",
      "display(Markdown(response))\n",
      "\n",
      "qa_stuff = RetrievalQA.from_chain_type(\n",
      "    llm=llm, \n",
      "    chain_type=\"stuff\", \n",
      "    retriever=retriever, \n",
      "    verbose=True\n",
      ")\n",
      "\n",
      "query =  (\"Please extract all shirts that mention sun protection. \"\n",
      "    \"Present them in a markdown table with columns: Name and Description. \"\n",
      "    \"Do not include any additional text or explanation. Only output the table.\"\n",
      "         )\n",
      "         \n",
      "response = qa_stuff.run(query)\n",
      "\n",
      "display(Markdown(response))\n",
      "Chain Type‚Ä¶\n",
      "‚Ä¶other than ‚Äústuff‚Äù\n",
      "[Image] file: \n",
      "Fun stuff\n",
      "In the original More detailed example code of the course the prompt and query were:\n",
      "prompt = f\"\"\"{qdocs} Question: Please list all your shirts with sun protection \\\n",
      "in a table in markdown and summarize each one.\"\"\"\n",
      "\n",
      "query =  \"\"\"Please list all shirts with sun protection in a table \\\n",
      "in markdown and summarize each one.\"\"\"\n",
      "\n",
      "When I tried to run the code I got response 523694 characters long that looked like this in Jupyter notebook:\n",
      "[Image] file: \n",
      "Upon further investigation I tried counting all characters‚Ä¶\n",
      "from collections import Counter\n",
      "char_counts = Counter(response)\n",
      "print(char_counts.most_common(10))\n",
      " ‚Ä¶and got this:\n",
      "[(' ', 523677), ('|', 2), ('e', 2), ('i', 2), ('N', 1), ('a', 1), ('m', 1), ('D', 1), ('s', 1), ('c', 1)]\n",
      "So model was heavily hallucinating because of the ambiguous prompt, returning 523677 spaces. \n",
      "Explanation:\n",
      "ChatGPT\n",
      "ChatGPT\n",
      " \n",
      "But why the first Less code example worked just fine? ü§®\n",
      "Because it used a different model:\n",
      "ChatGP\n",
      "üîç Under the Hood: What Went Wrong\n",
      "1. Vague Subject (\"your shirts\")\n",
      "The phrase \"your shirts\" is ambiguous ‚Äî is it referring to the model's own inventory?\n",
      "Since LLMs don't have memory of actual shirts, the model tries to make some up based on what it knows about \"shirts with sun protection\" ‚Äî possibly defaulting to hallucination mode.\n",
      "2. Open-Ended Prompt (\"list all\")\n",
      "Saying \"list all shirts\" with no cap tells the model:\n",
      "This often leads to runaway output ‚Äî which can be garbage, like repeated blank rows or excessive spaces.\n",
      "3. Markdown + Table Expectations\n",
      "Markdown tables follow a strict structure (|...|...|).\n",
      "But if the model gets confused about what data to put in, it might try to \"format blank rows\" ‚Äî which ends up being lots of empty lines or whitespace padding.\n",
      "4. Lack of Anchoring Context\n",
      "You asked a structured question, but didn't define what the data looks like or limit the answer.\n",
      "Without grounding, the model tries to ‚Äúfill in‚Äù blanks, and if unsure, it may default to empty structure with filler characters (e.g., spaces).\n",
      "‚ÄúGenerate as much as you can until max length is hit.‚Äù\n",
      "üõ† Fixing It ‚Äî What Helps the Model Behave\n",
      "To avoid this kind of behavior:\n",
      "‚úÖ Because gpt-3.5-turbo-instruct is fundamentally different from ChatOpenAI models like gpt-4, and it's better at handling classic instruction-following without hallucinating markdown structures.\n",
      "Let‚Äôs unpack why that code works well with your original prompt ‚Äî even though the ChatOpenAI + .call_as_llm() version doesn't.\n",
      "üß† Key Differences Between gpt-3.5-turbo-instruct and ChatOpenAI Models\n",
      "‚öôÔ∏è In Your Code:\n",
      "llm_replacement_model = OpenAI(temperature=0, model='gpt-3.5-turbo-instruct')\n",
      "response = index.query(query, llm = llm_replacement_model)\n",
      "This works well because:\n",
      "gpt-3.5-turbo-instruct treats your prompt literally ‚Äî like classic Codex or Davinci-style models.\n",
      "It sees list all your shirts with sun protection in a table in markdown and just outputs a compact, reasonable answer ‚Äî no extra \"chatty\" behavior.\n",
      "Its instruction-tuned nature works well with Retrieval-Augmented Generation (RAG) like index.query() because the context (qdocs) is very cleanly integrated into the prompt.\n",
      "It does not try to be clever or verbose ‚Äî just follows the instruction.\n",
      "üîÅ But With ChatOpenAI.call_as_llm(...)?\n",
      "Even though .call_as_llm() is supposed to let ChatOpenAI models behave like classic LLMs, under the hood, they're still trained differently:\n",
      "More verbose\n",
      "More padding (especially in markdown tables)\n",
      "Greater risk of hallucinating spaces, structure, or excessive detail\n",
      "May interpret ambiguous language (\"your shirts\") differently ‚Äî even emotionally, as in a conversation\n",
      "‚úÖ Best Practice\n",
      "If your task is:\n",
      "Focused\n",
      "Instruction-like\n",
      "Table or code generation\n",
      "Structured data extraction from documents\n",
      "Then gpt-3.5-turbo-instruct (or similar instruct-style models like text-davinci-003) often works better than chat-based ones ‚Äî unless you're using a very well-designed prompt template with ChatOpenAI.\n",
      "ü§î¬†Why:  to understand how effective the system we‚Äôve created is.\n",
      "üìù¬†Important libraries: \n",
      "from langchain.evaluation.qa import QAEvalChain\n",
      "from langchain.evaluation.qa import QAGenerateChain\n",
      "üë©‚Äçüíª¬†Code example:\n",
      "from langchain.chains import RetrievalQA\n",
      "from langchain.chat_models import ChatOpenAI\n",
      "from langchain.document_loaders import CSVLoader\n",
      "from langchain.indexes import VectorstoreIndexCreator\n",
      "from langchain.vectorstores import DocArrayInMemorySearch\n",
      "\n",
      "from langchain.evaluation.qa import QAEvalChain\n",
      "from langchain.evaluation.qa import QAGenerateChain\n",
      "\n",
      "# Create a Q&A system\n",
      "\n",
      "# load data\n",
      "file = 'OutdoorClothingCatalog_1000.csv'\n",
      "loader = CSVLoader(file_path=file)\n",
      "data = loader.load()\n",
      "\n",
      "# create index\n",
      "index = VectorstoreIndexCreator(\n",
      "    vectorstore_cls=DocArrayInMemorySearch\n",
      ").from_loaders([loader])\n",
      "\n",
      "# q&a\n",
      "llm = ChatOpenAI(temperature = 0.0, model=llm_model)\n",
      "qa = RetrievalQA.from_chain_type(\n",
      "    llm=llm, \n",
      "    chain_type=\"stuff\", \n",
      "    retriever=index.vectorstore.as_retriever(), \n",
      "    verbose=True,\n",
      "    chain_type_kwargs = {\n",
      "        \"document_separator\": \"<<<<>>>>>\"\n",
      "    }\n",
      ")\n",
      "# hard-coding examples\n",
      "examples = [\n",
      "    {\n",
      "        \"query\": \"Do the Cozy Comfort Pullover Set\\\n",
      "        have side pockets?\",\n",
      "        \"answer\": \"Yes\"\n",
      "    },\n",
      "    {\n",
      "        \"query\": \"What collection is the Ultra-Lofty \\\n",
      "        850 Stretch Down Hooded Jacket from?\",\n",
      "        \"answer\": \"The DownTek collection\"\n",
      "    }\n",
      "]\n",
      "\n",
      "# using llm to geenrate more examples\n",
      "example_gen_chain = QAGenerateChain.from_llm(ChatOpenAI(model=llm_model))\n",
      "new_examples = example_gen_chain.apply_and_parse(\n",
      "    [{\"doc\": t} for t in data[:5]]\n",
      ")\n",
      "examples += new_examples\n",
      "import langchain\n",
      "langchain.debug = True\n",
      "\n",
      "qa.run(examples[0][\"query\"])\n",
      "\n",
      "# Turn off the debug mode\n",
      "langchain.debug = False\n",
      "predictions = qa.apply(examples)\n",
      "\n",
      "llm = ChatOpenAI(temperature=0, model=llm_model)\n",
      "eval_chain = QAEvalChain.from_llm(llm)\n",
      "\n",
      "graded_outputs = eval_chain.evaluate(examples, predictions)\n",
      "\n",
      "for i, eg in enumerate(examples):\n",
      "    print(f\"Example {i}:\")\n",
      "    print(\"Question: \" + predictions[i]['query'])\n",
      "    print(\"Real Answer: \" + predictions[i]['answer'])\n",
      "    print(\"Predicted Answer: \" + predictions[i]['result'])\n",
      "    print(\"Predicted Grade: \" + graded_outputs[i]['text'])\n",
      "    print()\n",
      "üèéÔ∏è¬†Built-in LangChain tools\n",
      "ü§î¬†Why: use existing tools to solve specific tasks.\n",
      "üìù¬†Important libraries: \n",
      "from langchain.agents import load_tools, initialize_agent\n",
      "from langchain.agents import AgentType\n",
      "üë©‚Äçüíª¬†Code example:\n",
      "# Get a list of all available tools\n",
      "from langchain.agents.load_tools import get_all_tool_names\n",
      "get_all_tool_names()\n",
      "from langchain.agents import load_tools, initialize_agent\n",
      "from langchain.agents import AgentType\n",
      "from langchain.chat_models import ChatOpenAI\n",
      "\n",
      "llm = ChatOpenAI(temperature=0, model=llm_model)\n",
      "\n",
      "tools = load_tools([\"llm-math\",\"wikipedia\"], llm=llm)\n",
      "agent= initialize_agent(\n",
      "    tools, \n",
      "    llm, \n",
      "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,  # Great for building smart assistants that can use tools dynamically without examples \n",
      "    handle_parsing_errors=True,\n",
      "    verbose = True)\n",
      "    \n",
      "agent(\"What is the 25% of 300?\")\n",
      "\n",
      "question = \"\"\"Tom M. Mitchell is an American computer scientist \\\n",
      "and the Founders University Professor at Carnegie Mellon University (CMU)\\\n",
      "what book did he write?\"\"\"\n",
      "result = agent(question) \n",
      "üõ∫¬†Python Agent\n",
      "ü§î¬†Why: create an agent that can write, run, and collect output of the python code.\n",
      "üìù¬†Important libraries: \n",
      "from langchain.tools.python.tool import PythonREPLTool\n",
      "from langchain.python import PythonREPL\n",
      "üë©‚Äçüíª¬†Code example:\n",
      "from langchain.tools.python.tool import PythonREPLTool\n",
      "from langchain.agents.agent_toolkits import create_python_agent\n",
      "\n",
      "agent = create_python_agent(\n",
      "    llm,\n",
      "    tool=PythonREPLTool(),\n",
      "    verbose=True\n",
      ")\n",
      "\n",
      "customer_list = [[\"Harrison\", \"Chase\"], \n",
      "                 [\"Lang\", \"Chain\"],\n",
      "                 [\"Dolly\", \"Too\"],\n",
      "                 [\"Elle\", \"Elem\"], \n",
      "                 [\"Geoff\",\"Fusion\"], \n",
      "                 [\"Trance\",\"Former\"],\n",
      "                 [\"Jen\",\"Ayai\"]\n",
      "                ]\n",
      "                \n",
      "agent.run(f\"\"\"Sort these customers by \\\n",
      "last name and then first name \\\n",
      "and print the output: {customer_list}\"\"\") \n",
      "\n",
      "# to see detailed output of the chain\n",
      "import langchain\n",
      "langchain.debug=True\n",
      "agent.run(f\"\"\"Sort these customers by \\\n",
      "last name and then first name \\\n",
      "and print the output: {customer_list}\"\"\") \n",
      "langchain.debug=False\n",
      "üö†¬†Custom Tools\n",
      "ü§î¬†Why: solve a very specific task that can‚Äôt be solved by existing tools.\n",
      "üìù¬†Important libraries: \n",
      "from langchain.agents import Tool\n",
      "üë©‚Äçüíª¬†Code example:\n",
      "from langchain.agents import initialize_agent, AgentType\n",
      "from langchain.agents import Tool\n",
      "from langchain.chat_models import ChatOpenAI\n",
      "import datetime\n",
      "\n",
      "# Define the function logic\n",
      "def get_current_time(_):\n",
      "    now = datetime.datetime.now()\n",
      "    return now.strftime(\"%H:%M:%S\")\n",
      "\n",
      "# Wrap it as a LangChain tool\n",
      "time_tool = Tool(\n",
      "    name=\"GetTime\",\n",
      "    func=get_current_time,\n",
      "    description=\"Useful for when you want to know the current time. Input is ignored.\"\n",
      ")\n",
      "\n",
      "# Initialize LLM\n",
      "llm = ChatOpenAI(temperature=0)\n",
      "\n",
      "# Set up the agent with the custom tool\n",
      "agent = initialize_agent(\n",
      "    tools=[time_tool],\n",
      "    llm=llm,\n",
      "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
      "    verbose=True\n",
      ")\n",
      "\n",
      "# Ask a question that should trigger the tool\n",
      "agent.run(\"What time is it right now?\")\n",
      "Summary: Basics of LangChain\n",
      "[Child Page] Lesson 1: Document Loading\n",
      "[Child Page] Lesson 2: Document Splitting\n",
      "[Child Page] Lesson 3: Vectorstores and Embeddings\n",
      "[Child Page] Lesson 4: Retrieval\n",
      "[Child Page] Lesson 5: Question Answering\n",
      "[Child Page] Lesson 6: Chat\n",
      "ü§î¬†Why: How your data gets into LangChain. Doesn‚Äôt always work as expected, recommended for complex multi-doc workflows, LLM chaining.\n",
      "üß† When Is LangChain Useful for Loading?\n",
      "üìù¬†Important libraries: \n",
      "from langchain.document_loaders import PyPDFLoader\n",
      "from langchain.document_loaders.generic import GenericLoader,  FileSystemBlobLoader\n",
      "from langchain.document_loaders.parsers import OpenAIWhisperParser\n",
      "from langchain.document_loaders.blob_loaders.youtube_audio import YoutubeAudioLoader\n",
      "from langchain.document_loaders import WebBaseLoader\n",
      "üë©‚Äçüíª¬†Code example:\n",
      "\"\"\"\n",
      "PDF loading.\n",
      "\n",
      "Each page is a Document.\n",
      "A Document contains text (page_content) and metadata.\n",
      "\"\"\"\n",
      "\n",
      "from langchain.document_loaders import PyPDFLoader\n",
      "loader = PyPDFLoader(\"docs/cs229_lectures/MachineLearning-Lecture01.pdf\")\n",
      "pages = loader.load()\n",
      "\n",
      "\"\"\"\n",
      "YouTube Audio loading.\n",
      "\n",
      "Beaks on most of the videos.\n",
      "\"\"\"\n",
      "\n",
      "from langchain.document_loaders.generic import GenericLoader,  FileSystemBlobLoader\n",
      "from langchain.document_loaders.parsers import OpenAIWhisperParser\n",
      "from langchain.document_loaders.blob_loaders.youtube_audio import YoutubeAudioLoader\n",
      "\n",
      "url=\"https://www.youtube.com/watch?v=3mRvCF4qyTA\"\n",
      "save_dir=\"docs/youtube/\"\n",
      "loader = GenericLoader(\n",
      "#     YoutubeAudioLoader([url],save_dir),  # fetch from youtube\n",
      "    FileSystemBlobLoader(save_dir, glob=\"*.m4a\"),   #fetch locally\n",
      "    OpenAIWhisperParser()\n",
      ")\n",
      "docs = loader.load()\n",
      "\n",
      "\"\"\"\n",
      "URL loading.\n",
      "\n",
      "Loads text from a webpage.\n",
      "\"\"\"\n",
      "\n",
      "from langchain.document_loaders import WebBaseLoader\n",
      "\n",
      "loader = WebBaseLoader(\"https://github.com/basecamp/handbook/blob/master/titles-for-programmers.md\")\n",
      "docs = loader.load()\n",
      "print(docs[0].page_content[:500])\n",
      "LangChain‚Äôs loaders are fine when:\n",
      "You want a quick prototype and don‚Äôt mind complexity.\n",
      "You‚Äôre chaining loading + chunking + embedding into one pipeline.\n",
      "You‚Äôre already using LangChain heavily.\n",
      "ü§î¬†Why: Break content into manageable, embed-friendly chunks.\n",
      "[Image] file: \n",
      "üìù¬†Important libraries: \n",
      "from langchain.text_splitter import TokenTextSplitter\n",
      "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
      "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
      "üë©‚Äçüíª¬†Code example:\n",
      "from langchain.text_splitter import TokenTextSplitter\n",
      "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
      "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
      "\n",
      "chunk_size =26\n",
      "chunk_overlap = 4\n",
      "\n",
      "'''\n",
      "Character Splitting \n",
      "\n",
      "Description: Simple text splitter based purely on character count. \n",
      "Useful when performance is more important than intelligent boundaries.\n",
      "Can be customized.\n",
      "'''\n",
      "c_splitter = CharacterTextSplitter(\n",
      "    chunk_size=chunk_size,\n",
      "    chunk_overlap=chunk_overlap,\n",
      "    length_function=len  # defaults to len, can be changed to whatever you need\n",
      "    #length_function=lambda x: len(x.split())  # count words instead of characters\n",
      ")\n",
      "text2 = 'abcdefghijklmnopqrstuvwxyzabcdefg'\n",
      "r_splitter.split_text(text2)\n",
      "\n",
      "'''\n",
      "Recursive Character Splitting \n",
      "\n",
      "Description: Splits text recursively on a hierarchy of separators (\\n\\n, \\n, ., \n",
      "etc.) until it fits chunk_size. Recommended for generic text.\n",
      "'''\n",
      "\n",
      "some_text = \"\"\"When writing documents, writers will use document structure to group content. \\\n",
      "This can convey to the reader, which idea's are related. For example, closely related ideas \\\n",
      "are in sentances. Similar ideas are in paragraphs. Paragraphs form a document. \\n\\n  \\\n",
      "Paragraphs are often delimited with a carriage return or two carriage returns. \\\n",
      "Carriage returns are the \"backslash n\" you see embedded in this string. \\\n",
      "Sentences have a period at the end, but also, have a space.\\\n",
      "and words are separated by space.\"\"\"\n",
      "\n",
      "r_splitter = RecursiveCharacterTextSplitter(\n",
      "    chunk_size=150,\n",
      "    chunk_overlap=0,\n",
      "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"]  # \"(?<=\\. )\" means split after the dot\n",
      ")\n",
      "r_splitter.split_text(some_text)\n",
      "\n",
      "'''\n",
      "Token Splitting \n",
      "\n",
      "Description: Splits text based on token count, not character count.\n",
      "Useful when working with token-limited LLMs (e.g., OpenAI models), ensuring each \n",
      "chunk fits the token budget.\n",
      "'''\n",
      "\n",
      "text_splitter = TokenTextSplitter(chunk_size=1, \n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tchunk_overlap=0,\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tencoding_name=\"cl100k_base\"  # e.g. GPT-4/GPT-3.5 tokenizer\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#tokenizer=custom_tokenizer)\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "text1 = \"foo bar bazzyfoo\"\n",
      "\n",
      "'''\n",
      "Markdown Splitting \n",
      "\n",
      "Description: Splits Markdown files based on their header structure (#, ##, etc.), \n",
      "optionally capturing hierarchy.Great for structured Markdown documents like \n",
      "notebooks, blog posts, or docs.\n",
      "'''\n",
      "\n",
      "markdown_document = \"\"\"# Title\\n\\n \\\n",
      "## Chapter 1\\n\\n \\\n",
      "Hi this is Jim\\n\\n Hi this is Joe\\n\\n \\\n",
      "### Section \\n\\n \\\n",
      "Hi this is Lance \\n\\n \n",
      "## Chapter 2\\n\\n \\\n",
      "Hi this is Molly\"\"\"\n",
      "\n",
      "headers_to_split_on = [\n",
      "    (\"#\", \"Header 1\"),\n",
      "    (\"##\", \"Header 2\"),\n",
      "    (\"###\", \"Header 3\"),\n",
      "]\n",
      "\n",
      "markdown_splitter = MarkdownHeaderTextSplitter(\n",
      "    headers_to_split_on=headers_to_split_on\n",
      ")\n",
      "md_header_splits = markdown_splitter.split_text(markdown_document)\n",
      "üöòEmbeddings\n",
      "ü§î¬†Why: translate raw data (text, images etc.) into dense, meaningful numerical representations that machines can understand, compare, and manipulate.\n",
      "üìù¬†Important libraries: \n",
      "from langchain.embeddings.openai import OpenAIEmbeddings\n",
      "üë©‚Äçüíª¬†Code example:\n",
      "import numpy as np\n",
      "\n",
      "from langchain.embeddings.openai import OpenAIEmbeddings\n",
      "embedding = OpenAIEmbeddings()\n",
      "\n",
      "sentence1 = \"i like dogs\"\n",
      "sentence2 = \"i like canines\"\n",
      "sentence3 = \"the weather is ugly outside\"\n",
      "\n",
      "embedding1 = embedding.embed_query(sentence1)\n",
      "embedding2 = embedding.embed_query(sentence2)\n",
      "embedding3 = embedding.embed_query(sentence3)\n",
      "\n",
      "np.dot(embedding1, embedding2)  # 0.9631511809630346\n",
      "np.dot(embedding1, embedding3)  # 0.7702031371038216\n",
      "np.dot(embedding2, embedding3)  # 0.7590540629791649\n",
      "ü§®Why dot product?\n",
      "ChatGPT\n",
      "ü§ìAdditionally: cosine similarity\n",
      "ChatGPT\n",
      "üöîVectorstores\n",
      "ü§î¬†Why: A vector store is like a search engine for meaning. It helps AI systems find the most relevant knowledge, even if the words used don‚Äôt exactly match.\n",
      "‚úÖ What problems do vector stores solve?\n",
      "üì¶ Popular vector stores\n",
      "üìù¬†Important libraries: \n",
      "from langchain.vectorstores import Chroma  # in-memory vectorstore\n",
      "üë©‚Äçüíª¬†Code example:\n",
      "from langchain.vectorstores import Chroma\n",
      "\n",
      "from langchain.document_loaders import PyPDFLoader\n",
      "from langchain.embeddings.openai import OpenAIEmbeddings\n",
      "embedding = OpenAIEmbeddings()\n",
      "\n",
      "# Load PDF\n",
      "loaders = [\n",
      "    # Duplicate documents on purpose - messy data\n",
      "    PyPDFLoader(\"docs/cs229_lectures/MachineLearning-Lecture01.pdf\"),\n",
      "    PyPDFLoader(\"docs/cs229_lectures/MachineLearning-Lecture01.pdf\"),\n",
      "    PyPDFLoader(\"docs/cs229_lectures/MachineLearning-Lecture02.pdf\"),\n",
      "    PyPDFLoader(\"docs/cs229_lectures/MachineLearning-Lecture03.pdf\")\n",
      "]\n",
      "docs = []\n",
      "for loader in loaders:\n",
      "    docs.extend(loader.load())\n",
      "    \n",
      "# Split\n",
      "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
      "text_splitter = RecursiveCharacterTextSplitter(\n",
      "    chunk_size = 1500,\n",
      "    chunk_overlap = 150\n",
      ")\n",
      "\n",
      "splits = text_splitter.split_documents(docs)\n",
      "\n",
      "persist_directory = 'docs/chroma/'\n",
      "!rm -rf ./docs/chroma  # remove old database files if any\n",
      "\n",
      "vectordb = Chroma.from_documents(\n",
      "    documents=splits,\n",
      "    embedding=embedding,\n",
      "    persist_directory=persist_directory\n",
      ")\n",
      "\n",
      "print(vectordb._collection.count())\n",
      "\n",
      "# Similarity Search\n",
      "\n",
      "question = \"is there an email i can ask for help\"\n",
      "docs = vectordb.similarity_search(question,k=3)\n",
      "len(docs)\n",
      "docs[0].page_content\n",
      "\n",
      "\"\"\"\n",
      "Why you‚Äôd use .persist()\n",
      "You‚Äôve added documents or embeddings to the vector store.\n",
      "\n",
      "You want to save them permanently, so next time your app runs, you can load instead of rebuilding the index.\n",
      "\"\"\"\n",
      "vectordb.persist()\n",
      "üñºÔ∏èDiagrams\n",
      "[Image] file: \n",
      "[Image] file: \n",
      "The dot product (also called inner product) is a mathematical operation that takes two vectors and returns a single number.\n",
      "If you have two vectors:\n",
      "A = [a1, a2, ..., an]\n",
      "B = [b1, b2, ..., bn]\n",
      "The dot product is:\n",
      "A ‚Ä¢ B = a1*b1 + a2*b2 + ... + an*bn\n",
      "So it multiplies corresponding elements and sums the results.\n",
      "üìê What does it mean geometrically?\n",
      "It measures:\n",
      "How much do these two vectors point in the same direction?\n",
      "If they point in the same direction, the dot product is large and positive.\n",
      "If they are orthogonal (90¬∞), the dot product is 0.\n",
      "If they point in opposite directions, the dot product is negative.\n",
      "üîó Why does a higher dot product mean more similarity?\n",
      "When we use embeddings, we assume:\n",
      "Similar meanings ‚Üí vectors pointing in the same direction\n",
      "Different meanings ‚Üí vectors diverge\n",
      "So when you take the dot product of two embedding vectors:\n",
      "A higher result means the two texts are closer in meaning\n",
      "A lower (or negative) result means they are dissimila\n",
      "Suppose the embeddings are:\n",
      "\"cat\"     ‚Üí [0.1, 0.8, 0.1]\n",
      "\"dog\"     ‚Üí [0.2, 0.7, 0.1]\n",
      "\"carrot\"  ‚Üí [-0.4, 0.2, 0.9]\n",
      "Now compute:\n",
      "dot(cat, dog) ‚Üí 0.1√ó0.2 + 0.8√ó0.7 + 0.1√ó0.1 = 0.02 + 0.56 + 0.01 = 0.59\n",
      "dot(cat, carrot) ‚Üí 0.1√ó-0.4 + 0.8√ó0.2 + 0.1√ó0.9 = -0.04 + 0.16 + 0.09 = 0.21\n",
      "üëâ The higher score for cat and dog reflects greater semantic similarity.\n",
      "Cosine similarity measures the angle between two vectors, regardless of their length.\n",
      "It tells you how similar the direction of two vectors is ‚Äî not their size.\n",
      "üß† Why is this useful for embeddings?\n",
      "Embeddings (like from LLMs) are often high-dimensional vectors.\n",
      "When comparing their semantic similarity, we care about direction, not magnitude.\n",
      "Two embeddings with the same meaning may have different magnitudes (scaling), but their directions will be similar.\n",
      "So cosine similarity says:\n",
      "‚ÄúLet‚Äôs ignore how long these vectors are and just see if they‚Äôre pointing in the same direction.‚Äù\n",
      "‚öôÔ∏è Formula\n",
      "Given two vectors A and B:\n",
      "cos_sim(A, B) = (A ‚Ä¢ B) / (||A|| * ||B||)\n",
      "Where:\n",
      "A ‚Ä¢ B is the dot product\n",
      "||A|| and ||B|| are the magnitudes (lengths) of the vectors\n",
      "This normalizes the vectors before comparing them.\n",
      "üî¢ Example\n",
      "Let‚Äôs say:\n",
      "A = [1, 2]\n",
      "B = [2, 4]\n",
      "They point in the same direction.\n",
      "Dot product: 1*2 + 2*4 = 2 + 8 = 10\n",
      "Magnitudes: ||A|| = sqrt(1¬≤ + 2¬≤) = sqrt(5), ||B|| = sqrt(2¬≤ + 4¬≤) = sqrt(20)\n",
      "Cosine similarity: 10 / (sqrt(5) * sqrt(20)) = 1\n",
      "‚úÖ Cosine similarity = 1 ‚Üí same direction ‚Üí identical meaning\n",
      "üéØ Scale of Cosine Similarity\n",
      "‚úÖ When to use cosine similarity (vs dot product)\n",
      "üí¨ TL;DR\n",
      "Cosine similarity = angle between two vectors.\n",
      "High cosine similarity = vectors point in similar direction = more semantically similar.\n",
      "ü§î¬†Why: Techniques for finding and returning the most relevant pieces of information from a larger knowledge base (e.g., documents, PDFs, database) to use in a prompt.\n",
      "üèìComparison Table\n",
      "üìù¬†Important libraries: \n",
      "from langchain.retrievers import SVMRetriever\n",
      "from langchain.retrievers import TFIDFRetriever\n",
      "from langchain.retrievers import ContextualCompressionRetriever\n",
      "from langchain.chains.query_constructor.base import AttributeInfo\n",
      "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
      "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
      "üë©‚Äçüíª¬†Code example:\n",
      "from langchain.llms import OpenAI\n",
      "from langchain.vectorstores import Chroma\n",
      "from langchain.embeddings.openai import OpenAIEmbeddings\n",
      "from langchain.retrievers import ContextualCompressionRetriever\n",
      "from langchain.chains.query_constructor.base import AttributeInfo\n",
      "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
      "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
      "\n",
      "embedding = OpenAIEmbeddings()\n",
      "vectordb = Chroma(\n",
      "    persist_directory=persist_directory,\n",
      "    embedding_function=embedding\n",
      ")\n",
      "\n",
      "texts = [\n",
      "    \"\"\"The Amanita phalloides has a large and imposing epigeous (aboveground) fruiting body (basidiocarp).\"\"\",\n",
      "    \"\"\"A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.\"\"\",\n",
      "    \"\"\"A. phalloides, a.k.a Death Cap, is one of the most poisonous of all known mushrooms.\"\"\",\n",
      "]\n",
      "smalldb = Chroma.from_texts(texts, embedding=embedding)\n",
      "question = \"Tell me about all-white mushrooms with large fruiting bodies\"\n",
      "#___________________________________________\n",
      "\n",
      "# Most general-purpose retrieval\n",
      "smalldb.similarity_search(question, k=2)\n",
      "\n",
      "question = \"what did they say about regression in the third lecture?\"\n",
      "docs = vectordb.similarity_search(\n",
      "    question,\n",
      "    k=3,\n",
      "    filter={\"source\":\"docs/cs229_lectures/MachineLearning-Lecture03.pdf\"}\n",
      ")\n",
      "#___________________________________________\n",
      "\n",
      "# Prevent overlapping chunks in long docs\n",
      "smalldb.max_marginal_relevance_search(question,k=2, fetch_k=3)\n",
      "#___________________________________________\n",
      "\n",
      "# Smart, filter-aware search\n",
      "metadata_field_info = [\n",
      "    AttributeInfo(\n",
      "        name=\"source\",\n",
      "        description=\"The lecture the chunk is from, should be one of `docs/cs229_lectures/MachineLearning-Lecture01.pdf`, `docs/cs229_lectures/MachineLearning-Lecture02.pdf`, or `docs/cs229_lectures/MachineLearning-Lecture03.pdf`\",\n",
      "        type=\"string\",\n",
      "    ),\n",
      "    AttributeInfo(\n",
      "        name=\"page\",\n",
      "        description=\"The page from the lecture\",\n",
      "        type=\"integer\",\n",
      "    ),\n",
      "]\n",
      "\n",
      "document_content_description = \"Lecture notes\"\n",
      "llm = OpenAI(model='gpt-3.5-turbo-instruct', temperature=0)\n",
      "retriever = SelfQueryRetriever.from_llm(\n",
      "    llm,\n",
      "    vectordb,\n",
      "    document_content_description,\n",
      "    metadata_field_info,\n",
      "    verbose=True\n",
      ")\n",
      "\n",
      "docs = retriever.get_relevant_documents(question)\n",
      "#___________________________________________\n",
      "\n",
      "# Condenses and filters noisy results\n",
      "\n",
      "def pretty_print_docs(docs):\n",
      "    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]))\n",
      "#___________________________________________\n",
      "\n",
      "# Wrap our vectorstore\n",
      "llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo-instruct\")\n",
      "compressor = LLMChainExtractor.from_llm(llm)\n",
      "\n",
      "compression_retriever = ContextualCompressionRetriever(\n",
      "    base_compressor=compressor,\n",
      "    base_retriever=vectordb.as_retriever()\n",
      ")\n",
      "question = \"what did they say about matlab?\"\n",
      "compressed_docs = compression_retriever.get_relevant_documents(question)\n",
      "pretty_print_docs(compressed_docs)\n",
      "#___________________________________________\n",
      "\n",
      "# MMR + ContextualCompressionRetriever\n",
      "compression_retriever = ContextualCompressionRetriever(\n",
      "    base_compressor=compressor,\n",
      "    base_retriever=vectordb.as_retriever(search_type = \"mmr\")\n",
      ")\n",
      "question = \"what did they say about matlab?\"\n",
      "compressed_docs = compression_retriever.get_relevant_documents(question)\n",
      "pretty_print_docs(compressed_docs)\n",
      "#___________________________________________\n",
      "\n",
      "\n",
      "loader = PyPDFLoader(\"docs/cs229_lectures/MachineLearning-Lecture01.pdf\")\n",
      "pages = loader.load()\n",
      "all_page_text=[p.page_content for p in pages]\n",
      "joined_page_text=\" \".join(all_page_text)\n",
      "\n",
      "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1500,chunk_overlap = 150)\n",
      "splits = text_splitter.split_text(joined_page_text)\n",
      "#___________________________________________\n",
      "\n",
      "# When you have Q&A data or specific retrieval goals\n",
      "svm_retriever = SVMRetriever.from_texts(splits,embedding)\n",
      "\n",
      "question = \"What are major topics for this class?\"\n",
      "docs_svm=svm_retriever.get_relevant_documents(question)\n",
      "docs_svm[0]\n",
      "#___________________________________________\n",
      "\n",
      "# Small/simple datasets, classic IR, no embeddings\n",
      "tfidf_retriever = TFIDFRetriever.from_texts(splits)\n",
      "\n",
      "question = \"what did they say about matlab?\"\n",
      "docs_tfidf=tfidf_retriever.get_relevant_documents(question)\n",
      "docs_tfidf[0]\n",
      "‚Ñπ¬†Retrievals FAQ\n",
      "ChatGPT\n",
      "üîù Top Retrieval Methods Used in Real-Life Projects\n",
      "‚úÖ 1. similarity_search (Baseline Retriever)\n",
      "Use case: Almost all RAG systems start here.\n",
      "Why it's used: Fast, simple, often ‚Äúgood enough‚Äù if embeddings and chunking are done right.\n",
      "Enhance with: Better chunking, metadata filtering, hybrid search.\n",
      "‚úÖ 2. max_marginal_relevance_search (MMR)\n",
      "Use case: QA over long documents or knowledge bases.\n",
      "Why it's used: Prevents duplicate/redundant chunks; surfaces diverse ideas.\n",
      "Real-life: Used in enterprise QA bots and multi-doc summarization systems.\n",
      "‚úÖ 3. SelfQueryRetriever\n",
      "Use case: When users query data that has structured metadata (e.g., dates, tags, categories).\n",
      "Why it's used: Combines semantic search with metadata filtering using LLM's reasoning.\n",
      "Example: ‚ÄúShow me customer complaints about billing from Q1 2023.‚Äù\n",
      "‚úÖ 4. MultiQueryRetriever (LLM-generated query expansion)\n",
      "Use case: Improves recall when user queries are vague or under-specified.\n",
      "How it works: LLM generates multiple semantically equivalent queries ‚Üí retrieves more results.\n",
      "Real-life: Used in search portals, support bots, and scholarly research assistants.\n",
      "‚úÖ 5. ContextualCompressionRetriever\n",
      "Use case: Working with long, noisy documents or token-constrained LLMs.\n",
      "Why it's used: Uses an LLM to compress/rewrite retrieved content to only what's relevant.\n",
      "Real-life: Used in financial document QA, legal contracts, transcripts.\n",
      "‚úÖ 6. BM25Retriever (via Elasticsearch or Weaviate)\n",
      "Use case: Hybrid retrieval: combine semantic (embedding) and lexical (keyword) relevance.\n",
      "Why it's used: Embeddings fail on rare terms, names, exact matches ‚Äî BM25 helps bridge that.\n",
      "Real-life: Most enterprise search systems combine BM25 + vector search.\n",
      "‚úÖ 7. EnsembleRetriever / MultiVectorRetriever\n",
      "Use case: Combining multiple retrieval strategies (e.g., MMR + keyword + metadata).\n",
      "Why it's used: Improves robustness across a wide range of user queries.\n",
      "Real-life: High-accuracy production systems use ensembles of retrievers + reranking.\n",
      "‚úÖ 8. ParentDocumentRetriever\n",
      "Use case: When your original docs are large but you split them into small chunks.\n",
      "Why it's used: It returns the original large context (e.g., section, full doc) when a relevant chunk is hit.\n",
      "Real-life: Used in law/legal, policy, academic document RAG pipelines.\n",
      "‚úÖ 9. TimeWeightedVectorStoreRetriever (from langchain-experimental)\n",
      "Use case: Applications where recency matters (e.g., chatbots, logs).\n",
      "How it works: Applies a time-decay factor to make recent info more likely to be retrieved.\n",
      "Real-life: Memory systems in AI agents, chat histories, recommendation systems.\n",
      "‚úÖ 10. MetadataFilteringRetriever (not a standalone retriever, but pattern)\n",
      "Use case: Apply filter logic on metadata (e.g., {\"topic\": \"finance\", \"source\": \"email\"}).\n",
      "Why it's used: Narrows scope of search, improves accuracy.\n",
      "Combine with: SelfQueryRetriever, FAISS, Chroma, etc.\n",
      "üß† Bonus: How to Choose\n",
      "ü§î¬†Why: Make LLM answer questions based on your data.\n",
      "üìù¬†Important libraries: \n",
      "from langchain.chains import RetrievalQA\n",
      "üë©‚Äçüíª¬†Code example:\n",
      "from langchain.chains import RetrievalQA\n",
      "\n",
      "from langchain.vectorstores import Chroma\n",
      "from langchain.prompts import PromptTemplate\n",
      "from langchain.embeddings.openai import OpenAIEmbeddings\n",
      "\n",
      "persist_directory = 'docs/chroma/'\n",
      "embedding = OpenAIEmbeddings()\n",
      "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n",
      "\n",
      "qa_chain = RetrievalQA.from_chain_type(\n",
      "    llm,\n",
      "    retriever=vectordb.as_retriever()\n",
      ")\n",
      "\n",
      "result = qa_chain({\"query\": question})\n",
      "result[\"result\"]\n",
      "\n",
      "# Add prompt\n",
      "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. \n",
      "{context}\n",
      "Question: {question}\n",
      "Helpful Answer:\"\"\"\n",
      "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
      "\n",
      "# Run chain\n",
      "qa_chain = RetrievalQA.from_chain_type(\n",
      "    llm,\n",
      "    retriever=vectordb.as_retriever(),\n",
      "    return_source_documents=True,\n",
      "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
      ")\n",
      "\n",
      "‚õìÔ∏è‚Äçüí•Chain types\n",
      "[Image] file: \n",
      "qa_chain_mr = RetrievalQA.from_chain_type(\n",
      "    llm,\n",
      "    retriever=vectordb.as_retriever(),\n",
      "    chain_type=\"map_reduce\"\n",
      ")\n",
      "\n",
      "qa_chain_mr = RetrievalQA.from_chain_type(\n",
      "    llm,\n",
      "    retriever=vectordb.as_retriever(),\n",
      "    chain_type=\"refine\"\n",
      ")\n",
      "ü§î¬†Why: Connect all previous steps and add memory to make system remember previous messages.\n",
      "(Actually repeating info from Lesson 2: Memory)\n",
      "[Image] file: \n",
      "üë©‚Äçüíª¬†Code example:\n",
      "# Full chatbot code\n",
      "from langchain.embeddings.openai import OpenAIEmbeddings\n",
      "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
      "from langchain.vectorstores import DocArrayInMemorySearch\n",
      "from langchain.document_loaders import TextLoader\n",
      "from langchain.chains import RetrievalQA,  ConversationalRetrievalChain\n",
      "from langchain.memory import ConversationBufferMemory\n",
      "from langchain.chat_models import ChatOpenAI\n",
      "from langchain.document_loaders import TextLoader\n",
      "from langchain.document_loaders import PyPDFLoader\n",
      "\n",
      "def load_db(file, chain_type, k):\n",
      "    # load documents\n",
      "    loader = PyPDFLoader(file)\n",
      "    documents = loader.load()\n",
      "    # split documents\n",
      "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
      "    docs = text_splitter.split_documents(documents)\n",
      "    # define embedding\n",
      "    embeddings = OpenAIEmbeddings()\n",
      "    # create vector database from data\n",
      "    db = DocArrayInMemorySearch.from_documents(docs, embeddings)\n",
      "    # define retriever\n",
      "    retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": k})\n",
      "    # create a chatbot chain. Memory is managed externally.\n",
      "    qa = ConversationalRetrievalChain.from_llm(\n",
      "        llm=ChatOpenAI(model_name=llm_name, temperature=0), \n",
      "        chain_type=chain_type, \n",
      "        retriever=retriever, \n",
      "        return_source_documents=True,\n",
      "        return_generated_question=True,\n",
      "    )\n",
      "    return qa \n",
      "\n",
      "import panel as pn\n",
      "import param\n",
      "\n",
      "class cbfs(param.Parameterized):\n",
      "    chat_history = param.List([])\n",
      "    answer = param.String(\"\")\n",
      "    db_query  = param.String(\"\")\n",
      "    db_response = param.List([])\n",
      "    \n",
      "    def __init__(self,  **params):\n",
      "        super(cbfs, self).__init__( **params)\n",
      "        self.panels = []\n",
      "        self.loaded_file = \"docs/cs229_lectures/MachineLearning-Lecture01.pdf\"\n",
      "        self.qa = load_db(self.loaded_file,\"stuff\", 4)\n",
      "    \n",
      "    def call_load_db(self, count):\n",
      "        if count == 0 or file_input.value is None:  # init or no file specified :\n",
      "            return pn.pane.Markdown(f\"Loaded File: {self.loaded_file}\")\n",
      "        else:\n",
      "            file_input.save(\"temp.pdf\")  # local copy\n",
      "            self.loaded_file = file_input.filename\n",
      "            button_load.button_style=\"outline\"\n",
      "            self.qa = load_db(\"temp.pdf\", \"stuff\", 4)\n",
      "            button_load.button_style=\"solid\"\n",
      "        self.clr_history()\n",
      "        return pn.pane.Markdown(f\"Loaded File: {self.loaded_file}\")\n",
      "\n",
      "    def convchain(self, query):\n",
      "        if not query:\n",
      "            return pn.WidgetBox(pn.Row('User:', pn.pane.Markdown(\"\", width=600)), scroll=True)\n",
      "        result = self.qa({\"question\": query, \"chat_history\": self.chat_history})\n",
      "        self.chat_history.extend([(query, result[\"answer\"])])\n",
      "        self.db_query = result[\"generated_question\"]\n",
      "        self.db_response = result[\"source_documents\"]\n",
      "        self.answer = result['answer'] \n",
      "        self.panels.extend([\n",
      "            pn.Row('User:', pn.pane.Markdown(query, width=600)),\n",
      "            pn.Row('ChatBot:', pn.pane.Markdown(self.answer, width=600, style={'background-color': '#F6F6F6'}))\n",
      "        ])\n",
      "        inp.value = ''  #clears loading indicator when cleared\n",
      "        return pn.WidgetBox(*self.panels,scroll=True)\n",
      "\n",
      "    @param.depends('db_query ', )\n",
      "    def get_lquest(self):\n",
      "        if not self.db_query :\n",
      "            return pn.Column(\n",
      "                pn.Row(pn.pane.Markdown(f\"Last question to DB:\", styles={'background-color': '#F6F6F6'})),\n",
      "                pn.Row(pn.pane.Str(\"no DB accesses so far\"))\n",
      "            )\n",
      "        return pn.Column(\n",
      "            pn.Row(pn.pane.Markdown(f\"DB query:\", styles={'background-color': '#F6F6F6'})),\n",
      "            pn.pane.Str(self.db_query )\n",
      "        )\n",
      "\n",
      "    @param.depends('db_response', )\n",
      "    def get_sources(self):\n",
      "        if not self.db_response:\n",
      "            return \n",
      "        rlist=[pn.Row(pn.pane.Markdown(f\"Result of DB lookup:\", styles={'background-color': '#F6F6F6'}))]\n",
      "        for doc in self.db_response:\n",
      "            rlist.append(pn.Row(pn.pane.Str(doc)))\n",
      "        return pn.WidgetBox(*rlist, width=600, scroll=True)\n",
      "\n",
      "    @param.depends('convchain', 'clr_history') \n",
      "    def get_chats(self):\n",
      "        if not self.chat_history:\n",
      "            return pn.WidgetBox(pn.Row(pn.pane.Str(\"No History Yet\")), width=600, scroll=True)\n",
      "        rlist=[pn.Row(pn.pane.Markdown(f\"Current Chat History variable\", styles={'background-color': '#F6F6F6'}))]\n",
      "        for exchange in self.chat_history:\n",
      "            rlist.append(pn.Row(pn.pane.Str(exchange)))\n",
      "        return pn.WidgetBox(*rlist, width=600, scroll=True)\n",
      "\n",
      "    def clr_history(self,count=0):\n",
      "        self.chat_history = []\n",
      "        return \n",
      "\n",
      "cb = cbfs()\n",
      "\n",
      "file_input = pn.widgets.FileInput(accept='.pdf')\n",
      "button_load = pn.widgets.Button(name=\"Load DB\", button_type='primary')\n",
      "button_clearhistory = pn.widgets.Button(name=\"Clear History\", button_type='warning')\n",
      "button_clearhistory.on_click(cb.clr_history)\n",
      "inp = pn.widgets.TextInput( placeholder='Enter text here‚Ä¶')\n",
      "\n",
      "bound_button_load = pn.bind(cb.call_load_db, button_load.param.clicks)\n",
      "conversation = pn.bind(cb.convchain, inp) \n",
      "\n",
      "jpg_pane = pn.pane.Image( './img/convchain.jpg')\n",
      "\n",
      "tab1 = pn.Column(\n",
      "    pn.Row(inp),\n",
      "    pn.layout.Divider(),\n",
      "    pn.panel(conversation,  loading_indicator=True, height=300),\n",
      "    pn.layout.Divider(),\n",
      ")\n",
      "tab2= pn.Column(\n",
      "    pn.panel(cb.get_lquest),\n",
      "    pn.layout.Divider(),\n",
      "    pn.panel(cb.get_sources ),\n",
      ")\n",
      "tab3= pn.Column(\n",
      "    pn.panel(cb.get_chats),\n",
      "    pn.layout.Divider(),\n",
      ")\n",
      "tab4=pn.Column(\n",
      "    pn.Row( file_input, button_load, bound_button_load),\n",
      "    pn.Row( button_clearhistory, pn.pane.Markdown(\"Clears chat history. Can use to start a new topic\" )),\n",
      "    pn.layout.Divider(),\n",
      "    pn.Row(jpg_pane.clone(width=400))\n",
      ")\n",
      "dashboard = pn.Column(\n",
      "    pn.Row(pn.pane.Markdown('# ChatWithYourData_Bot')),\n",
      "    pn.Tabs(('Conversation', tab1), ('Database', tab2), ('Chat History', tab3),('Configure', tab4))\n",
      ")\n",
      "\n",
      "dashboard\n"
     ]
    }
   ],
   "source": [
    "page_id = \"21615fcdd071807889b4c29a7f8a54b6\"  # copy from URL or Notion client\n",
    "text = extract_page_text(notion, page_id)\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8098ff8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_block_children(notion, block_id):\n",
    "    children = []\n",
    "    cursor = None\n",
    "    while True:\n",
    "        response = notion.blocks.children.list(block_id=block_id, start_cursor=cursor)\n",
    "        children.extend(response[\"results\"])\n",
    "        cursor = response.get(\"next_cursor\")\n",
    "        if not cursor:\n",
    "            break\n",
    "    return children\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b436eae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_block(block):\n",
    "    block_type = block[\"type\"]\n",
    "    text = \"\"\n",
    "\n",
    "    if block_type in [\"paragraph\", \"heading_1\", \"heading_2\", \"heading_3\",\n",
    "                      \"bulleted_list_item\", \"numbered_list_item\", \"quote\", \"callout\", \"to_do\"]:\n",
    "        for rt in block[block_type].get(\"rich_text\", []):\n",
    "            text += rt.get(\"plain_text\", \"\")\n",
    "    elif block_type == \"child_page\":\n",
    "        text += f\"[Child Page] {block['child_page']['title']}\"\n",
    "    elif block_type == \"child_database\":\n",
    "        text += f\"[Database] {block['child_database']['title']}\"\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "174f6531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse_page(notion, block_id, depth=0):\n",
    "    content = []\n",
    "    blocks = get_block_children(notion, block_id)\n",
    "\n",
    "    for block in blocks:\n",
    "        prefix = \"  \" * depth\n",
    "        # text = extract_text_from_block(block)\n",
    "        # if text:\n",
    "        #     content.append(f\"{prefix}- {text}\")\n",
    "\n",
    "        # Recursively handle nested blocks\n",
    "        if block.get(\"has_children\"):\n",
    "            content.extend(traverse_page(notion, block[\"id\"], depth + 1))\n",
    "\n",
    "        # Special case: child_database ‚Äì query it\n",
    "        if block[\"type\"] == \"child_database\":\n",
    "            db_id = block[\"id\"]\n",
    "            try:\n",
    "                db_pages = get_pages_from_database(notion, db_id)\n",
    "                for p in db_pages:\n",
    "                    title = extract_page_title(p)\n",
    "                    content.append(f\"{prefix}  - üóÇÔ∏è Database Page: {title}\")\n",
    "                    content.extend(traverse_page(notion, p[\"id\"], depth + 2))\n",
    "            except Exception as e:\n",
    "                print(str(e))\n",
    "    \n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4494fce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pages_from_database(notion, database_id):\n",
    "    pages = []\n",
    "    cursor = None\n",
    "    while True:\n",
    "        response = notion.databases.query(database_id=database_id, start_cursor=cursor)\n",
    "        pages.extend(response[\"results\"])\n",
    "        cursor = response.get(\"next_cursor\")\n",
    "        if not cursor:\n",
    "            break\n",
    "    return pages\n",
    "\n",
    "def extract_page_title(page):\n",
    "    props = page.get(\"properties\", {})\n",
    "    for val in props.values():\n",
    "        if val[\"type\"] == \"title\":\n",
    "            return \"\".join([t[\"plain_text\"] for t in val[\"title\"]])\n",
    "    return \"[No Title]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "477aacac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find database with ID: 22a15fcd-d071-808c-ab19-cc42fad5a7a5. Make sure the relevant pages and databases are shared with your integration.\n",
      "      - üóÇÔ∏è Database Page: Finnish  LangChain: Chat with Your Data \n",
      "      - üóÇÔ∏è Database Page: Try to find better interview preparation materials\n",
      "      - üóÇÔ∏è Database Page: Collect all comments for movie-success-predictor\n",
      "      - üóÇÔ∏è Database Page: Finnish at least 3 lessons in  LangChain: Chat with Your Data \n",
      "      - üóÇÔ∏è Database Page: Day 1 of interview-question-data-science \n",
      "      - üóÇÔ∏è Database Page: Finnish LangChain for LLM Application Development \n",
      "      - üóÇÔ∏è Database Page: Start running data collection for movie-success-predictor\n",
      "  - üóÇÔ∏è Database Page: Data Science Interview Questions and Answers\n",
      "  - üóÇÔ∏è Database Page: LangChain: Chat with Your Data\n",
      "  - üóÇÔ∏è Database Page: LangChain for LLM Application Development\n",
      "  - üóÇÔ∏è Database Page: Prompt Engineering for Vision Models\n",
      "  - üóÇÔ∏è Database Page: ChatGPT Prompt Engineering for Developers\n",
      "  - üóÇÔ∏è Database Page: Let's build the GPT Tokenizer (Andrej Karpathy)\n",
      "  - üóÇÔ∏è Database Page: Let's build GPT: from scratch, in code, spelled out. (Andrej Karpathy)\n",
      "  - üóÇÔ∏è Database Page: Let's reproduce GPT-2 (124M) (Andrej Karpathy)\n",
      "  - üóÇÔ∏è Database Page: Data Science Source Materials (Quantum)\n",
      "  - üóÇÔ∏è Database Page: interview-question-data-science\n",
      "  - üóÇÔ∏è Database Page: Intro to Large Language Models (Andrej Karpathy)\n",
      "  - üóÇÔ∏è Database Page: SoftServe Academy\n"
     ]
    }
   ],
   "source": [
    "page_id = \"21615fcdd071807889b4c29a7f8a54b6\"\n",
    "\n",
    "content = traverse_page(notion, page_id)\n",
    "print(\"\\n\".join(content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "84efc8cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "APIResponseError",
     "evalue": "Could not find database with ID: 22a15fcd-d071-808c-ab19-cc42fad5a7a5. Make sure the relevant pages and databases are shared with your integration.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/site-packages/notion_client/client.py:118\u001b[0m, in \u001b[0;36mBaseClient._parse_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/site-packages/httpx/_models.py:829\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    828\u001b[0m message \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m, error_type\u001b[38;5;241m=\u001b[39merror_type)\n\u001b[0;32m--> 829\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request\u001b[38;5;241m=\u001b[39mrequest, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPStatusError\u001b[0m: Client error '404 Not Found' for url 'https://api.notion.com/v1/databases/22a15fcdd071808cab19cc42fad5a7a5/query'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAPIResponseError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnotion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatabases\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatabase_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m22a15fcdd071808cab19cc42fad5a7a5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/site-packages/notion_client/api_endpoints.py:134\u001b[0m, in \u001b[0;36mDatabasesEndpoint.query\u001b[0;34m(self, database_id, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquery\u001b[39m(\u001b[38;5;28mself\u001b[39m, database_id: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SyncAsync[Any]:\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a list of [Pages](https://developers.notion.com/reference/page) contained in the database.\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m    *[üîó Endpoint documentation](https://developers.notion.com/reference/post-database-query)*\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdatabases/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdatabase_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/query\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpick\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfilter_properties\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfilter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msorts\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstart_cursor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpage_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marchived\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43min_trash\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/site-packages/notion_client/client.py:194\u001b[0m, in \u001b[0;36mClient.request\u001b[0;34m(self, path, method, query, body, auth)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException:\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestTimeoutError()\n\u001b[0;32m--> 194\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.7/lib/python3.12/site-packages/notion_client/client.py:126\u001b[0m, in \u001b[0;36mBaseClient._parse_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    124\u001b[0m         code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m code \u001b[38;5;129;01mand\u001b[39;00m is_api_error_code(code):\n\u001b[0;32m--> 126\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m APIResponseError(response, body[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m], code)\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPResponseError(error\u001b[38;5;241m.\u001b[39mresponse)\n\u001b[1;32m    129\u001b[0m body \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "\u001b[0;31mAPIResponseError\u001b[0m: Could not find database with ID: 22a15fcd-d071-808c-ab19-cc42fad5a7a5. Make sure the relevant pages and databases are shared with your integration."
     ]
    }
   ],
   "source": [
    "notion.databases.query(database_id=\"22a15fcdd071808cab19cc42fad5a7a5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90f48f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
